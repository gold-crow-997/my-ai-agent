{
  "nodes": [
    {
      "parameters": {
        "fieldToSplitOut": "urls",
        "options": {}
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        1408,
        288
      ],
      "id": "f4f79841-b13b-4ffc-a800-26398e20104e",
      "name": "Split URLs from Final Sitemap",
      "notes": "Splits the final URL array into individual items for processing"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "1702b5bd-c55e-44fb-ae44-cae92d909258",
              "leftValue": "={{ $json.type === 'url' }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        1632,
        288
      ],
      "id": "b15dad01-e3d9-4061-b366-8d8223131c4e",
      "name": "Filter Only URL Types",
      "notes": "Keeps only actual URLs, excludes sitemap references"
    },
    {
      "parameters": {
        "aggregate": "aggregateAllItemData",
        "options": {}
      },
      "type": "n8n-nodes-base.aggregate",
      "typeVersion": 1,
      "position": [
        2080,
        288
      ],
      "id": "cf778e58-f0db-4320-ac9e-f19042cc4489",
      "name": "Aggregate All URLs",
      "notes": "Combines all final URLs into a single output array"
    },
    {
      "parameters": {
        "assignments": {
          "assignments": [
            {
              "id": "17055ad9-9fef-4313-a6cd-186702b28409",
              "name": "url",
              "value": "={{ $json.loc }}",
              "type": "string"
            },
            {
              "id": "30e8bcec-9013-4666-bfe0-d5c341349aac",
              "name": "lastmod",
              "value": "={{ $json.lastmod }}",
              "type": "string"
            },
            {
              "id": "612739f3-a721-4029-8ab5-e03b17bcfca9",
              "name": "priority",
              "value": "={{ $json.priority }}",
              "type": "string"
            },
            {
              "id": "c3486a15-56a4-40aa-bebe-7e89804ecf5d",
              "name": "changefreq",
              "value": "={{ $json.changefreq }}",
              "type": "string"
            },
            {
              "id": "f859d9fd-021b-4a8a-8a0d-6f7860034f24",
              "name": "parentSitemap",
              "value": "={{ $json.parentSitemap }}",
              "type": "string"
            }
          ]
        },
        "options": {}
      },
      "type": "n8n-nodes-base.set",
      "typeVersion": 3.4,
      "position": [
        1856,
        288
      ],
      "id": "7169c9d7-132c-4068-a683-b3940bad367c",
      "name": "Normalize URL Fields",
      "notes": "Renames 'loc' to 'url' and formats output structure"
    },
    {
      "parameters": {
        "url": "={{ $json.sitemaps }}",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {}
          ]
        },
        "options": {
          "redirect": {
            "redirect": {}
          },
          "response": {
            "response": {
              "fullResponse": true
            }
          }
        }
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [
        512,
        176
      ],
      "id": "70a63d92-763f-4dcf-81d3-dc5efbb42015",
      "name": "Fetch Sitemap XML",
      "retryOnFail": true,
      "notesInFlow": true,
      "onError": "continueErrorOutput",
      "notes": "Downloads the sitemap XML file from the provided URL"
    },
    {
      "parameters": {
        "jsCode": "// Robust XML Sitemap Parser for n8n\n// Handles: standard sitemaps, sitemap indexes, news sitemaps, image sitemaps, video sitemaps\n// Input: expects $input.item.json.data to contain XML content\n\n  try {\n  const xmlContent = String($input.item.json.data || '').trim();\n  const currentDepth = $input.item.json.sitemapDepth || 0;\n  const maxDepth = $('Receive Sitemap Input').first().json.maxCrawlDepth // Prevent infinite loops\n  \n  if (currentDepth > maxDepth) {\n    return {\n      json: {\n        success: false,\n        error: 'Max sitemap depth exceeded',\n        maxDepthReached: true\n      }\n    };\n  }\n\n  // Helper function to extract text from XML tags\n const extractTag = (xml, tagName) => {\n    const regex = new RegExp(`<${tagName}[^>]*>([\\\\s\\\\S]*?)<\\\\/${tagName}>`, 'gi');\n    const matches = [];\n    let match;\n    while ((match = regex.exec(xml)) !== null) {\n      matches.push(match[1].trim());\n    }\n    return matches;\n  };\n\n  // Helper function to extract single tag value\n  const extractSingleTag = (xml, tagName) => {\n    const regex = new RegExp(`<${tagName}[^>]*>([\\\\s\\\\S]*?)<\\\\/${tagName}>`, 'i');\n    const match = xml.match(regex);\n    return match ? match[1].trim() : null;\n  };\n\n  // Determine sitemap type\n  let sitemapType = 'unknown';\n  if (xmlContent.includes('<sitemapindex')) {\n    sitemapType = 'sitemapindex';\n  } else if (xmlContent.includes('<urlset')) {\n    sitemapType = 'urlset';\n  }\n\n  const result = {\n    type: sitemapType,\n    urls: [],\n    sitemapDepth: currentDepth,\n    parentSitemap: $input.item.json.sitemapUrl || $input.item.json['parsed.sitemaps'] || 'unknown',\n    metadata: {\n      sourceUrl: $input.item.json.sitemapUrl || $input.item.json['parsed.sitemaps'] || 'unknown',\n      parsedAt: new Date().toISOString(),\n      totalUrls: 0,\n      depth: currentDepth\n    }\n  };\n\n  // Parse Sitemap Index (contains links to other sitemaps)\n  if (sitemapType === 'sitemapindex') {\n    const sitemaps = extractTag(xmlContent, 'sitemap');\n    \n    result.urls = sitemaps.map(sitemap => {\n      return {\n        loc: extractSingleTag(sitemap, 'loc'),\n        lastmod: extractSingleTag(sitemap, 'lastmod'),\n        type: 'sitemap',\n        sitemapDepth: currentDepth + 1 // Increment depth for child sitemaps\n      };\n    }).filter(item => item.loc);\n  }\n  \n  // Parse URL Set (contains actual page URLs)\n  else if (sitemapType === 'urlset') {\n    const urls = extractTag(xmlContent, 'url');\n    \n    result.urls = urls.map(url => {\n      const urlData = {\n        loc: extractSingleTag(url, 'loc'),\n        lastmod: extractSingleTag(url, 'lastmod'),\n        changefreq: extractSingleTag(url, 'changefreq'),\n        priority: extractSingleTag(url, 'priority'),\n        type: 'url',\n        parentSitemap: result.parentSitemap\n      };\n\n      // Parse news sitemap data if present\n      if (url.includes('<news:news>')) {\n        const newsData = extractSingleTag(url, 'news:news');\n        if (newsData) {\n          urlData.news = {\n            publicationName: extractSingleTag(newsData, 'news:name'),\n            publicationLanguage: extractSingleTag(newsData, 'news:language'),\n            publicationDate: extractSingleTag(newsData, 'news:publication_date'),\n            title: extractSingleTag(newsData, 'news:title')\n          };\n        }\n      }\n\n      // Parse image data if present\n      const images = extractTag(url, 'image:image');\n      if (images.length > 0) {\n        urlData.images = images.map(img => ({\n          loc: extractSingleTag(img, 'image:loc'),\n          caption: extractSingleTag(img, 'image:caption'),\n          title: extractSingleTag(img, 'image:title')\n        }));\n      }\n\n      // Parse video data if present\n      const videos = extractTag(url, 'video:video');\n      if (videos.length > 0) {\n        urlData.videos = videos.map(vid => ({\n          thumbnailLoc: extractSingleTag(vid, 'video:thumbnail_loc'),\n          title: extractSingleTag(vid, 'video:title'),\n          description: extractSingleTag(vid, 'video:description'),\n          contentLoc: extractSingleTag(vid, 'video:content_loc')\n        }));\n      }\n\n      return urlData;\n    }).filter(item => item.loc);\n  }\n\n  // Update metadata\n  result.metadata.totalUrls = result.urls.length;\n\n  const summary = {\n    totalUrls: result.urls.length,\n    sitemapType: sitemapType,\n    depth: currentDepth,\n    isSitemapIndex: sitemapType === 'sitemapindex'\n  };\n\n  return {\n    json: {\n      success: true,\n      error: null,\n      ...result,\n      summary: summary\n    }\n  };\n\n} catch (error) {\n  return {\n    json: {\n      success: false,\n      error: error.message || 'Unknown parsing error',\n      errorStack: error.stack || null,\n      type: null,\n      urls: []\n    }\n  };\n}"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        736,
        96
      ],
      "id": "91e20d07-8287-4ed5-96f2-907c42ec30b2",
      "name": "Parse XML Sitemap",
      "notes": "Parses XML content and extracts URLs/sitemaps with metadata (news, images, videos)"
    },
    {
      "parameters": {
        "jsCode": "const item = $input.item.json;\n\nreturn {\n  json: {\n    ...item,\n    isSitemapIndex: item.type === 'sitemapindex',\n    needsRecursion: item.type === 'sitemapindex' && item.urls && item.urls.length > 0\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1008,
        96
      ],
      "id": "a90b0ebe-220a-4496-9a97-328a948f71c4",
      "name": "Check Recursion Needed",
      "notes": "Determines if sitemap contains child sitemaps requiring further processing"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "cf4579ea-f77d-40e2-be88-f25ca355c7aa",
              "leftValue": "={{ $json.needsRecursion }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [
        1200,
        96
      ],
      "id": "c12adaec-3894-46b7-92e1-a2aaff746b0f",
      "name": "Is Sitemap Index?",
      "notes": "Routes to recursion (TRUE) or final URL extraction (FALSE)"
    },
    {
      "parameters": {
        "jsCode": "// Convert all formats to {\"sitemaps\": \"url\", \"sitemapDepth\": n}\n\nconst item = $input.item.json;\n\nlet sitemapUrl = null;\nlet sitemapDepth = 0;\n\n// Handle different input formats\nif (item['parsed.sitemaps']) {\n  sitemapUrl = item['parsed.sitemaps'];\n  sitemapDepth = item.sitemapDepth || 0;\n}\nelse if (item.loc) {\n  sitemapUrl = item.loc;\n  sitemapDepth = item.sitemapDepth || 0;\n}\n\nreturn {\n  json: {\n    'sitemaps': sitemapUrl,\n    sitemapDepth: sitemapDepth\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1632,
        96
      ],
      "id": "309c0c39-ee83-4714-bc9b-4ca64b4da360",
      "name": "Normalize for Recursion",
      "notes": "Formats child sitemap data for recursive processing (loops back to start)"
    },
    {
      "parameters": {
        "inputSource": "jsonExample",
        "jsonExample": "{\n  \"maxCrawlDepth\": 5,\n  \"sitemaps\": \"https://example.com/sitemap.xml\",\n  \"sitemapDepth\": 0,\n  \"robotsRules\": {}\n}"
      },
      "id": "5759e2ef-4989-4f43-a4fc-5119af3b6b6e",
      "typeVersion": 1.1,
      "name": "Receive Sitemap Input",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "position": [
        288,
        176
      ],
      "notes": "Entry point: receives sitemap URL, depth, and max crawl depth from parent workflow"
    },
    {
      "parameters": {
        "fieldToSplitOut": "urls",
        "options": {}
      },
      "type": "n8n-nodes-base.splitOut",
      "typeVersion": 1,
      "position": [
        1408,
        96
      ],
      "id": "c375d295-78ad-45c8-947e-5ad027198996",
      "name": "Split Child Sitemaps",
      "notes": "Splits sitemap index into individual child sitemap URLs"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict",
            "version": 2
          },
          "conditions": [
            {
              "id": "e0c9108f-620e-4a22-9034-1b2d759f083c",
              "leftValue": "={{ $json.status === 'allowed' }}",
              "rightValue": "",
              "operator": {
                "type": "boolean",
                "operation": "true",
                "singleValue": true
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.filter",
      "typeVersion": 2.2,
      "position": [
        2528,
        288
      ],
      "id": "4ff21912-e175-44cd-98e4-0709ed4ce98b",
      "name": "Keep Only Allowed URLs",
      "notes": "Filters out excluded and blocked URLs, keeps only allowed ones"
    },
    {
      "parameters": {
        "content": "## ðŸŽ¯ Input & Initialization\n**Purpose:** Receives sitemap URL and configuration from parent workflow\n\n**Nodes:**\n- Receive Sitemap Input\n\n**Input Parameters:**\n- `sitemaps`: Sitemap URL to process\n- `sitemapDepth`: Current recursion depth (0 = root)\n- `maxCrawlDepth`: Maximum allowed depth (prevents infinite loops)\n- `robotsRules`: Parsed robots.txt rules\npatterns\n\n**Output:** Configuration passed to fetch stage",
        "height": 608,
        "width": 368,
        "color": 6
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        160,
        -288
      ],
      "typeVersion": 1,
      "id": "95ccfd5d-d361-4a57-86b5-939217f18875",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "content": "## ðŸ“¥ Sitemap Fetching & Parsing\n**Purpose:** Downloads and parses XML sitemap content\n\n**Nodes:**\n- Fetch Sitemap XML\n- Parse XML Sitemap\n- Check Recursion Needed\n\n**Features:**\n- Handles HTTP errors gracefully (continues on error)\n- Supports multiple sitemap formats:\n  - Standard URL sets (`<urlset>`)\n  - Sitemap indexes (`<sitemapindex>`)\n  - News sitemaps (`<news:news>`)\n  - Image sitemaps (`<image:image>`)\n  - Video sitemaps (`<video:video>`)\n- Extracts metadata (lastmod, priority, changefreq)\n\n**Output:** Parsed sitemap with type detection (index vs urlset)",
        "height": 752,
        "width": 400,
        "color": 3
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        544,
        -416
      ],
      "typeVersion": 1,
      "id": "6068a593-abd8-438d-9a70-a551c79f3ba5",
      "name": "Sticky Note1"
    },
    {
      "parameters": {
        "content": "## ðŸ”„ Recursion Routing\n**Purpose:** Routes sitemap indexes to recursive processing, URL sets to final extraction\n\n**Nodes:**\n- Is Sitemap Index?\n- Split Child Sitemaps\n- Normalize for Recursion\n\n**Logic:**\n- **TRUE (Sitemap Index):**\n  - Splits child sitemap URLs\n  - Normalizes format (`{sitemaps, sitemapDepth}`)\n  - **Loops back** to \"Fetch Sitemap XML\" (recursion)\n  - Increments `sitemapDepth` by 1\n  \n- **FALSE (URL Set):**\n  - Proceeds to URL extraction\n  - No further recursion needed\n\n**Depth Protection:** Stops if `sitemapDepth > maxCrawlDepth`",
        "height": 960,
        "width": 608
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        960,
        -528
      ],
      "typeVersion": 1,
      "id": "a8eb1425-030a-4308-9152-a718ea5cdfea",
      "name": "Sticky Note2"
    },
    {
      "parameters": {
        "content": "## ðŸ”— URL Extraction & Normalization\n**Purpose:** Extracts final URLs from sitemap and normalizes field names\n\n**Nodes:**\n- Split URLs from Final Sitemap\n- Filter Only URL Types\n- Normalize URL Fields\n- Aggregate All URLs\n\n**Processing:**\n1. Splits URL array into individual items\n2. Filters out sitemap references (keeps only `type: 'url'`)\n3. Renames `loc` â†’ `url` for consistency\n4. Preserves metadata (lastmod, priority, changefreq, parentSitemap)\n5. Aggregates all URLs into single output array\n\n**Output:** Clean array of URLs with metadata",
        "height": 976,
        "width": 624,
        "color": 7
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        1584,
        -528
      ],
      "typeVersion": 1,
      "id": "fb6c87fc-01b0-4b66-acc3-ca7b8bf353c0",
      "name": "Sticky Note3"
    },
    {
      "parameters": {
        "content": "## ðŸš« Filtering & Output\n**Purpose:** Applies robots.txt and custom exclusion rules to final URL list\n\n**Nodes:**\n- Apply Robots Filters\n- Keep Only Allowed URLs\n\n**Filtering Rules:**\n1. **Exclusion List Patterns:**\n   - Supports wildcards (`/admin/*`, `*.pdf`)\n   - Matches full URLs or paths\n   - Comma-separated patterns\n\n**Status Flags:**\n- `allowed`: Passes all filters\n- `blocked_by_robots`: Disallowed by robots.txt\n\n**Final Output:** Only URLs with `status: 'allowed'` returned to parent workflow",
        "height": 976,
        "width": 544,
        "color": 4
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        2224,
        -528
      ],
      "typeVersion": 1,
      "id": "3d4500f9-60cd-464b-a1c5-0ceaf608a54d",
      "name": "Sticky Note4"
    },
    {
      "parameters": {
        "jsCode": "// Filter URLs based on robots.txt only\n// Exclusion list logic has been removed as it's no longer needed\n\nconst robotsRules = $('Receive Sitemap Input').first()?.json?.robotsRules?.userAgents || [];\nconst items = $input.all();\nconst filteredUrls = [];\n\n// Helper: Check if URL is disallowed by robots.txt\nfunction isDisallowedByRobots(url, userAgents) {\n  if (!userAgents || userAgents.length === 0) {\n    return { blocked: false };\n  }\n  \n  try {\n    const urlPath = new URL(url).pathname;\n    \n    const wildcardAgent = userAgents.find(ua => ua.name === '*');\n    if (!wildcardAgent || !wildcardAgent.disallow) {\n      return { blocked: false };\n    }\n    \n    // Find most specific matching rules\n    let mostSpecificDisallow = null;\n    let mostSpecificAllow = null;\n    \n    // Check all disallow rules\n    for (const disallowPath of wildcardAgent.disallow) {\n      if (disallowPath && urlPath.startsWith(disallowPath)) {\n        if (!mostSpecificDisallow || disallowPath.length > mostSpecificDisallow.length) {\n          mostSpecificDisallow = disallowPath;\n        }\n      }\n    }\n    \n    // Check all allow rules\n    if (wildcardAgent.allow) {\n      for (const allowPath of wildcardAgent.allow) {\n        if (allowPath && urlPath.startsWith(allowPath)) {\n          if (!mostSpecificAllow || allowPath.length > mostSpecificAllow.length) {\n            mostSpecificAllow = allowPath;\n          }\n        }\n      }\n    }\n    \n    // Most specific rule wins\n    if (mostSpecificDisallow && mostSpecificAllow) {\n      // If both match, the longer (more specific) one wins\n      if (mostSpecificDisallow.length > mostSpecificAllow.length) {\n        return { \n          blocked: true, \n          reason: `Disallowed by robots.txt: ${mostSpecificDisallow} (overrides Allow: ${mostSpecificAllow})` \n        };\n      } else {\n        return { blocked: false };\n      }\n    }\n    \n    // Only disallow matched\n    if (mostSpecificDisallow) {\n      return { \n        blocked: true, \n        reason: `Disallowed by robots.txt: ${mostSpecificDisallow}` \n      };\n    }\n    \n    // No disallow rules matched\n    return { blocked: false };\n    \n  } catch (e) {\n    console.error('Error checking robots.txt:', e.message);\n    return { blocked: false };\n  }\n}\n\n// Filter URLs based on robots.txt only\nfor (const item of items) {\n  const itemData = item.json;\n  const urlsArray = itemData.data || [itemData];\n  \n  for (const urlData of urlsArray) {\n    const url = urlData.url;\n    \n    if (!url) continue;\n    \n    console.log(`\\n=== Processing URL: ${url} ===`);\n    \n    // Check robots.txt rules\n    const robotsCheck = isDisallowedByRobots(url, robotsRules);\n    console.log('Robots check result:', robotsCheck);\n    \n    if (robotsCheck.blocked) {\n      filteredUrls.push({\n        ...urlData,\n        filtered: true,\n        filterReason: robotsCheck.reason,\n        status: 'blocked_by_robots'\n      });\n      continue;\n    }\n    \n    // URL is allowed\n    console.log('URL ALLOWED');\n    filteredUrls.push({\n      ...urlData,\n      filtered: false,\n      filterReason: null,\n      status: 'allowed'\n    });\n  }\n}\n\nconsole.log('\\n=== Final filtered URLs count:', filteredUrls.length);\nconsole.log('Blocked by robots.txt:', filteredUrls.filter(u => u.status === 'blocked_by_robots').length);\nconsole.log('Allowed count:', filteredUrls.filter(u => u.status === 'allowed').length);\n\nreturn filteredUrls.map(url => ({ json: url }));"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2304,
        288
      ],
      "id": "764dd7cb-23ed-4b2d-a6ae-7019995882df",
      "name": "Apply Robots Filters",
      "onError": "continueRegularOutput",
      "notes": "Applies robots.txt and custom exclusion patterns to all URLs"
    }
  ],
  "connections": {
    "Split URLs from Final Sitemap": {
      "main": [
        [
          {
            "node": "Filter Only URL Types",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Filter Only URL Types": {
      "main": [
        [
          {
            "node": "Normalize URL Fields",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate All URLs": {
      "main": [
        [
          {
            "node": "Apply Robots Filters",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize URL Fields": {
      "main": [
        [
          {
            "node": "Aggregate All URLs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Sitemap XML": {
      "main": [
        [
          {
            "node": "Parse XML Sitemap",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse XML Sitemap": {
      "main": [
        [
          {
            "node": "Check Recursion Needed",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check Recursion Needed": {
      "main": [
        [
          {
            "node": "Is Sitemap Index?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Is Sitemap Index?": {
      "main": [
        [
          {
            "node": "Split Child Sitemaps",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Split URLs from Final Sitemap",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Normalize for Recursion": {
      "main": [
        [
          {
            "node": "Fetch Sitemap XML",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Receive Sitemap Input": {
      "main": [
        [
          {
            "node": "Fetch Sitemap XML",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Child Sitemaps": {
      "main": [
        [
          {
            "node": "Normalize for Recursion",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Apply Robots Filters": {
      "main": [
        [
          {
            "node": "Keep Only Allowed URLs",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "pinData": {},
  "meta": {
    "instanceId": "c7824031308db534c7c9e014216a02a2cfe1378532902e3ebe0cf84de7a241ad"
  }
}